{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d10d99db",
   "metadata": {},
   "source": [
    "# WEBSCRAPING ASSIGNMENT 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54c556f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\garym\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: requests in c:\\users\\garym\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\garym\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: regex in c:\\users\\garym\\anaconda3\\lib\\site-packages (2022.7.9)\n",
      "Requirement already satisfied: pip in c:\\users\\garym\\anaconda3\\lib\\site-packages (23.2.1)\n",
      "Requirement already satisfied: selenium in c:\\users\\garym\\anaconda3\\lib\\site-packages (4.23.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\garym\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\garym\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\garym\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\garym\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\garym\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\garym\\anaconda3\\lib\\site-packages (from requests) (2024.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\garym\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\garym\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\garym\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\garym\\anaconda3\\lib\\site-packages (from selenium) (0.26.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\garym\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\garym\\anaconda3\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\garym\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\garym\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\garym\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\garym\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\garym\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\garym\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\garym\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\garym\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\garym\\anaconda3\\lib\\site-packages (from urllib3<3,>=1.21.1->requests) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\garym\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\garym\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "# preparing python for webscraping\n",
    "!pip install beautifulsoup4 requests pandas regex pip selenium numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "715dcf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.edge.options import Options\n",
    "import random\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ce806b",
   "metadata": {},
   "source": [
    "# 1. Write a python program which searches all the product under a particular product from www.amazon.in. Theproduct to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5f049f",
   "metadata": {},
   "source": [
    "### In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. \n",
    "### In case if any product has less than 3 pages in search results then scrape all the products available under that product name. \n",
    "### Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fd2b5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution completed.\n",
      "Execution completed.\n",
      "193\n",
      "193\n",
      "193\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Brand Description</th>\n",
       "      <th>Price in rupees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon Brand - Symbol</td>\n",
       "      <td>Men's Cotton Shirt | Casual | Plain | Full Sle...</td>\n",
       "      <td>549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KINGDOM OF WHITE</td>\n",
       "      <td>Svetah 100% Cotton Solid White Shirt I Full Sl...</td>\n",
       "      <td>1,149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MARK &amp; ALBERT</td>\n",
       "      <td>Men's Half Sleeve Linen Shirt</td>\n",
       "      <td>910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KINGDOM OF WHITE</td>\n",
       "      <td>Root in 100% Cotton White Kurta Shirt for Men ...</td>\n",
       "      <td>1,249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CB-COLEBROOK</td>\n",
       "      <td>Men's Casual Button Down Shirts Long Sleeve Li...</td>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Arrow</td>\n",
       "      <td>Men's Solid Full Sleeve Slim Fit Cutaway Colla...</td>\n",
       "      <td>1,039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>FRENCH CROWN</td>\n",
       "      <td>Men's Beige Pure Cotton Regular Fit Solid Full...</td>\n",
       "      <td>2,997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Arrow</td>\n",
       "      <td>Men's Checkered Full Sleeve Slim Fit Point Col...</td>\n",
       "      <td>1,452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Thomas Scott</td>\n",
       "      <td>Mens Half Sleeves Cotton Blend Crochet Texture...</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>GLORYBOYZ</td>\n",
       "      <td>Men's Big Checkered Shirt for Men Classic Coll...</td>\n",
       "      <td>849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Brand Name                                  Brand Description  \\\n",
       "0    Amazon Brand - Symbol  Men's Cotton Shirt | Casual | Plain | Full Sle...   \n",
       "1         KINGDOM OF WHITE  Svetah 100% Cotton Solid White Shirt I Full Sl...   \n",
       "2            MARK & ALBERT                      Men's Half Sleeve Linen Shirt   \n",
       "3         KINGDOM OF WHITE  Root in 100% Cotton White Kurta Shirt for Men ...   \n",
       "4             CB-COLEBROOK  Men's Casual Button Down Shirts Long Sleeve Li...   \n",
       "..                     ...                                                ...   \n",
       "188                  Arrow  Men's Solid Full Sleeve Slim Fit Cutaway Colla...   \n",
       "189           FRENCH CROWN  Men's Beige Pure Cotton Regular Fit Solid Full...   \n",
       "190                  Arrow  Men's Checkered Full Sleeve Slim Fit Point Col...   \n",
       "191           Thomas Scott  Mens Half Sleeves Cotton Blend Crochet Texture...   \n",
       "192              GLORYBOYZ  Men's Big Checkered Shirt for Men Classic Coll...   \n",
       "\n",
       "    Price in rupees  \n",
       "0               549  \n",
       "1             1,149  \n",
       "2               910  \n",
       "3             1,249  \n",
       "4               499  \n",
       "..              ...  \n",
       "188           1,039  \n",
       "189           2,997  \n",
       "190           1,452  \n",
       "191             514  \n",
       "192             849  \n",
       "\n",
       "[193 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver = webdriver.Edge()\n",
    "\n",
    "driver.get('https://www.amazon.in/')\n",
    "\n",
    "#create variables for data\n",
    "all_brands_class_1 = []\n",
    "all_brands_class_2 = []\n",
    "all_prices_class_1 = []\n",
    "\n",
    "# variable to discover the search bar\n",
    "search_box = WebDriverWait(driver, 30).until(\n",
    "    EC.element_to_be_clickable((By.ID, 'twotabsearchtextbox'))\n",
    ")\n",
    "\n",
    "# send the 'Shirt' input\n",
    "search_box.send_keys('Shirt')\n",
    "\n",
    "# Click the search button\n",
    "search_button = WebDriverWait(driver, 30).until(\n",
    "    EC.element_to_be_clickable((By.ID, 'nav-search-submit-button'))\n",
    ")\n",
    "search_button.click()\n",
    "\n",
    "#creating the variables to store names, description, price\n",
    "\n",
    "brand_names_class_1 = WebDriverWait(driver, 30).until(\n",
    "    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"span.a-size-base-plus.a-color-base:not(.a-text-normal)\"))\n",
    ")\n",
    "\n",
    "brand_names_class_2 = WebDriverWait(driver, 30).until(\n",
    "    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"span.a-size-base-plus.a-color-base.a-text-normal\"))\n",
    ")\n",
    "\n",
    "price_elements_class_3 = WebDriverWait(driver, 30).until(\n",
    "    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"span.a-price-whole\"))\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create the variables for the lists of shirts\n",
    "brands_class_1 = [link.text for link in brand_names_class_1 if link.text]\n",
    "brands_class_2 = [link.text for link in brand_names_class_2 if link.text]\n",
    "prices_class_3 = [price.text for price in price_elements_class_3 if price.text]\n",
    "                                         \n",
    "                                         \n",
    "# extend the data in the variables\n",
    "all_brands_class_1.extend(brands_class_1)\n",
    "all_brands_class_2.extend(brands_class_2)\n",
    "all_prices_class_1.extend(prices_class_3)\n",
    "\n",
    "# this is my attempt to save csv files\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('brands_class_1.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Brand Name'])  # Header\n",
    "    for name in brands_class_1:\n",
    "        writer.writerow([name])\n",
    "\n",
    "with open('brands_class_2.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Brand Name'])  # Header\n",
    "    for name in brands_class_2:\n",
    "        writer.writerow([name])\n",
    "\n",
    "# this has stopped working for some reason\n",
    "        \n",
    "#with open('prices_class_3.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "#    writer = csv.writer(file)\n",
    "#    writer.writerow(['Price'])  # Header\n",
    "#    for price in prices_class_1:\n",
    "#        writer.writerow([price])\n",
    "\n",
    "try:\n",
    "    # try block to automate loading the next page\n",
    "    #variable to load next page looking for correct button\n",
    "    next_page_button = WebDriverWait(driver, 30).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \"a.s-pagination-item.s-pagination-next\"))\n",
    "    )\n",
    "    # Click the button\n",
    "    next_page_button.click()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    print(\"Execution completed.\") \n",
    "\n",
    "    \n",
    "# some sleep to let the page load\n",
    "time.sleep(15)\n",
    "\n",
    "# variables to store data in next page\n",
    "brand_names_class_4 = WebDriverWait(driver, 50).until(\n",
    "    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"span.a-size-base-plus.a-color-base:not(.a-text-normal)\"))\n",
    ")\n",
    "\n",
    "brand_names_class_5 = WebDriverWait(driver, 50).until(\n",
    "    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"span.a-size-base-plus.a-color-base.a-text-normal\"))\n",
    ")\n",
    "\n",
    "price_elements_class_6 = WebDriverWait(driver, 50).until(\n",
    "    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"span.a-price-whole\"))\n",
    ")\n",
    "\n",
    "\n",
    "# again extracting the text for the lists\n",
    "brands_class_4 = [link.text for link in brand_names_class_4 if link.text]\n",
    "brands_class_5 = [link.text for link in brand_names_class_5 if link.text]\n",
    "prices_class_6 = [price.text for price in price_elements_class_6 if price.text]\n",
    "                                         \n",
    "                                         \n",
    "#adding the new variable data to the list\n",
    "all_brands_class_1.extend(brands_class_4)\n",
    "all_brands_class_2.extend(brands_class_5)\n",
    "all_prices_class_1.extend(prices_class_6)\n",
    "\n",
    "\n",
    "# i can't work out how to create the prices_class CSV files\n",
    "import csv\n",
    "\n",
    "with open('brands_class_4.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Brand Name'])  # Header\n",
    "    for name in brands_class_1:\n",
    "        writer.writerow([name])\n",
    "\n",
    "with open('brands_class_5.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Brand Name'])  # Header\n",
    "    for name in brands_class_2:\n",
    "        writer.writerow([name])\n",
    "        \n",
    "#with open('prices_class_6.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "#    writer = csv.writer(file)\n",
    "#    writer.writerow(['Price'])  # Header\n",
    "#    for price in prices_class_1:\n",
    "#        writer.writerow([price])\n",
    "\n",
    "\n",
    "\n",
    "# use the try block to load the next page\n",
    "try:\n",
    "    # Wait for the element to be clickable\n",
    "    next_page_button = WebDriverWait(driver, 30).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \"a.s-pagination-item.s-pagination-next\"))\n",
    "    )\n",
    "    # Click the button\n",
    "    next_page_button.click()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    print(\"Execution completed.\") \n",
    "    \n",
    "time.sleep(15)\n",
    "    \n",
    "# storing the data in the variables for page 3\n",
    "brand_names_class_7 = WebDriverWait(driver, 30).until(\n",
    "    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"span.a-size-base-plus.a-color-base:not(.a-text-normal)\"))\n",
    ")\n",
    "\n",
    "brand_names_class_8 = WebDriverWait(driver, 30).until(\n",
    "    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"span.a-size-base-plus.a-color-base.a-text-normal\"))\n",
    ")\n",
    "\n",
    "price_elements_class_9 = WebDriverWait(driver, 30).until(\n",
    "    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"span.a-price-whole\"))\n",
    ")\n",
    "\n",
    "\n",
    "# collect the data into variables for the lists\n",
    "brands_class_7 = [link.text for link in brand_names_class_7 if link.text]\n",
    "brands_class_8 = [link.text for link in brand_names_class_8 if link.text]\n",
    "prices_class_9 = [price.text for price in price_elements_class_9 if price.text]\n",
    "                                         \n",
    "                                         \n",
    "# add the variables to the original data\n",
    "all_brands_class_1.extend(brands_class_7)\n",
    "all_brands_class_2.extend(brands_class_8)\n",
    "all_prices_class_1.extend(prices_class_9)\n",
    "\n",
    "\n",
    "# still trying to save the correct csv files\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('brands_class_1.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Brand Name'])  # Header\n",
    "    for name in brands_class_1:\n",
    "        writer.writerow([name])\n",
    "\n",
    "with open('brands_class_2.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Brand Name'])  # Header\n",
    "    for name in brands_class_2:\n",
    "        writer.writerow([name])\n",
    "        \n",
    "#with open('prices_class_1.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "#    writer = csv.writer(file)\n",
    "#    writer.writerow(['Price'])  # Header\n",
    "#    for price in prices_class_1:\n",
    "#        writer.writerow([price])\n",
    "    \n",
    "\n",
    "\n",
    "# check the lengths of the variables\n",
    "\n",
    "print(len(all_brands_class_1))\n",
    "print(len(all_brands_class_2))\n",
    "print(len(all_prices_class_1))\n",
    "\n",
    "\n",
    "# checking the lengths of the columns\n",
    "max_length = max(len(all_brands_class_1), len(all_brands_class_2), len(all_prices_class_1))\n",
    "\n",
    "all_brands_class_1 += [None] * (max_length - len(all_brands_class_1))\n",
    "all_brands_class_2 += [None] * (max_length - len(all_brands_class_2))\n",
    "all_prices_class_1 += [None] * (max_length - len(all_prices_class_1))\n",
    "\n",
    "#preparing the data for the lists\n",
    "data = {\n",
    "    'Brand Name': all_brands_class_1,\n",
    "    'Brand Description': all_brands_class_2,\n",
    "    'Price in rupees': all_prices_class_1\n",
    "}\n",
    "# creating the dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# i am not sure how to place the n/a value in the correct space\n",
    "df['Brand Description'].fillna('N/A', inplace=True)\n",
    "\n",
    "# trying to make the lists the same length\n",
    "df['Price in rupees'].fillna(0, inplace=True)  \n",
    "(df)\n",
    "\n",
    "driver.quit()\n",
    "# I have tried several times to place an N/A value in the dataframe, \n",
    "# however i cannot guarantee the data accuracy as i don't know how to \n",
    "# place the N/A in the correct row. However sometimes Amazon.IN lists \n",
    "# are equal length\n",
    "# I Haven't been able to webscrape the other datas as i've run out of time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3970cd99",
   "metadata": {},
   "source": [
    "# Q3. Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0854b30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Edge()\n",
    "driver.get('https://images.google.com/')\n",
    "\n",
    "# create a variable to click the accept cookies button\n",
    "accept_button = WebDriverWait(driver, 30).until(\n",
    "    EC.presence_of_element_located((By.CSS_SELECTOR, \".QS5gu.sy4vM\"))\n",
    ")\n",
    "\n",
    "# Scroll down to the button\n",
    "driver.execute_script(\"arguments[0].scrollIntoView();\", accept_button)\n",
    "\n",
    "# Wait until the button is clickable\n",
    "accept_button = WebDriverWait(driver, 30).until(\n",
    "    EC.element_to_be_clickable((By.CSS_SELECTOR, \".QS5gu.sy4vM\"))\n",
    ")\n",
    "\n",
    "\n",
    "accept_button.click()\n",
    "\n",
    "# Give the page some time to load\n",
    "time.sleep(5)\n",
    "\n",
    "#create a variable to use the search bar \n",
    "search_box = WebDriverWait(driver, 30).until(\n",
    "    EC.element_to_be_clickable((By.ID, 'APjFqb'))\n",
    ")\n",
    "# send the search request\n",
    "search_box.send_keys('fruits')\n",
    "\n",
    "# click the button\n",
    "search_button = WebDriverWait(driver, 30).until(\n",
    "    EC.element_to_be_clickable((By.CLASS_NAME, \"HZVG1b\"))\n",
    ")\n",
    "search_button.click()\n",
    "\n",
    "# Wait\n",
    "time.sleep(5)\n",
    "\n",
    "# Get the page source and parse it\n",
    "soup3 = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# find the links to the pictures\n",
    "img_tags = soup3.find_all(attrs={'data-lpage': True})\n",
    "\n",
    "# create the fruits variable.\n",
    "fruits = []\n",
    "\n",
    "# count variable\n",
    "count = 0\n",
    "\n",
    "# Loop and save the information found on the page\n",
    "for img in img_tags:\n",
    "    if count < 10:  \n",
    "        fruits.append(img.get('data-lpage'))  \n",
    "        count += 1  # Increment the counter\n",
    "    else:\n",
    "        break  # Exit the loop after 10 results\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Now repeat the same steps as above with a different search variable\n",
    "search_box = WebDriverWait(driver, 30).until(\n",
    "    EC.element_to_be_clickable((By.ID, 'APjFqb'))\n",
    ")\n",
    "\n",
    "search_box.clear()\n",
    "\n",
    "search_box.send_keys('cars')\n",
    "\n",
    "search_button = WebDriverWait(driver, 30).until(\n",
    "    EC.element_to_be_clickable((By.CLASS_NAME, \"HZVG1b\"))\n",
    ")\n",
    "search_button.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "soup4 = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "\n",
    "# create new variables for soup and img_tags\n",
    "img_tags1 = soup4.find_all(attrs={'data-lpage': True})\n",
    "\n",
    "# cars variable\n",
    "cars = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "for img1 in img_tags1:\n",
    "    if count < 10:  # Check if the counter is less than 10\n",
    "        cars.append(img1.get('data-lpage'))  # Append the data-lpage value to the list\n",
    "        count += 1  # Increment the counter\n",
    "    else:\n",
    "        break  # Exit the loop after 10 results\n",
    "\n",
    "time.sleep(5)\n",
    "# Wait for the search input element to be present and clickable\n",
    "search_box = WebDriverWait(driver, 30).until(\n",
    "    EC.element_to_be_clickable((By.ID, 'APjFqb'))\n",
    ")        \n",
    "\n",
    "search_box.clear()\n",
    "        \n",
    "search_box.send_keys('Machine Learning')\n",
    "\n",
    "search_button = WebDriverWait(driver, 30).until(\n",
    "    EC.element_to_be_clickable((By.CLASS_NAME, \"HZVG1b\"))\n",
    ")\n",
    "search_button.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# new vairables\n",
    "soup5 = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "img_tags2 = soup5.find_all(attrs={'data-lpage': True})\n",
    "\n",
    "# machine learning variable\n",
    "Machine_Learning = set()\n",
    "# for loop to collect 10 datas\n",
    "for img2 in img_tags2:\n",
    "    if len(Machine_Learning) < 10:  \n",
    "        Machine_Learning.add(img2.get('data-lpage')) \n",
    "    else:\n",
    "        break \n",
    "\n",
    "\n",
    "Machine_Learning = list(Machine_Learning)\n",
    "\n",
    "time.sleep(5)\n",
    "# new search for guitars\n",
    "search_box = WebDriverWait(driver, 30).until(\n",
    "    EC.element_to_be_clickable((By.ID, 'APjFqb'))\n",
    ")\n",
    "search_box.clear()\n",
    "\n",
    "search_box.send_keys('Guitar')\n",
    "\n",
    "search_button = WebDriverWait(driver, 30).until(\n",
    "    EC.element_to_be_clickable((By.CLASS_NAME, \"HZVG1b\"))\n",
    ")\n",
    "search_button.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# new variables for soup and images\n",
    "soup6 = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "img_tags3 = soup6.find_all(attrs={'data-lpage': True})\n",
    "\n",
    "# guitar list\n",
    "Guitar = []\n",
    "\n",
    "# count variable\n",
    "count = 0\n",
    "\n",
    "# for loop to find data\n",
    "for img3 in img_tags3:\n",
    "    if count < 10:  \n",
    "        Guitar.append(img3.get('data-lpage'))  \n",
    "        count += 1  \n",
    "    else:\n",
    "        break  \n",
    "        \n",
    "# time.sleep(5)\n",
    "# search_box = WebDriverWait(driver, 30).until(\n",
    "#    EC.element_to_be_clickable((By.ID, 'APjFqb'))\n",
    "# )\n",
    "\n",
    "# search_box.clear()\n",
    "# search_box.send_keys('Cakes')\n",
    "\n",
    "# Locate and click the search button\n",
    "# search_button = WebDriverWait(driver, 30).until(\n",
    "#    EC.element_to_be_clickable((By.CLASS_NAME, \"HZVG1b\"))\n",
    "#)\n",
    "#search_button.click()\n",
    "#time.sleep(5)\n",
    "#soup4 = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "# result_count = 0\n",
    "# max_results = 10  # Set the maximum number of results you want\n",
    "\n",
    "# the cakes html class is confusing me\n",
    "# for link in soup4.find_all('a', class_='iGVLpd kGQAp BqKtob lNHeqe'):\n",
    "#    if result_count < max_results:\n",
    "#        product_url = link.get('href')\n",
    "#        product_title = link.get('title') or link.get('aria-label')\n",
    "#        if product_url and product_title:\n",
    "#\n",
    "#print(f'Product URL: {product_url}, Product Title: {product_title}')\n",
    "#            result_count += 1\n",
    "#    else:\n",
    "#        break  # Exit the loop once 10 results are processed\n",
    "\n",
    "\n",
    "print(fruits)\n",
    "print(cars)\n",
    "print(Machine_Learning)\n",
    "print(Guitar)\n",
    "# print(Cakes)\n",
    "driver.quit()\n",
    "# I have not been able to webscrape the images to the desktop as i do not know how to do that, \n",
    "# when i tried to look for cakes i ran out of time.\n",
    "# Here is a list of webpages that link to the images requested in the question.\n",
    "# Thanks for reading this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41e37b5",
   "metadata": {},
   "source": [
    "# Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com.\n",
    "## Scrape following details for all the search results displayed on 1st page. \n",
    "\n",
    "## Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. \n",
    "\n",
    "## Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521bd107",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Edge()\n",
    "\n",
    "# load Flipkart\n",
    "driver.get('https://www.flipkart.com/')\n",
    "\n",
    "# accept cookies\n",
    "search_form = WebDriverWait(driver, 30).until(\n",
    "    EC.presence_of_element_located((By.CSS_SELECTOR, \"form._2rslOn.header-form-search\"))\n",
    ")\n",
    "\n",
    "search_input = search_form.find_element(By.CSS_SELECTOR, 'input[name=\"q\"]')\n",
    "\n",
    "driver.execute_script(\"arguments[0].scrollIntoView();\", search_input)\n",
    "\n",
    "# Wait for the search field to be ready\n",
    "WebDriverWait(driver, 30).until(\n",
    "    EC.element_to_be_clickable((By.CSS_SELECTOR, 'input[name=\"q\"]'))\n",
    ")\n",
    "\n",
    "# search for the smartphone\n",
    "search_input.send_keys('smartphone')\n",
    "\n",
    "# enter the search and go full screen\n",
    "search_form.submit()  \n",
    "driver.fullscreen_window()\n",
    "\n",
    "time.sleep(15)\n",
    "# webscrape the data in KzDlHZ\n",
    "brand_elements = WebDriverWait(driver, 30).until(\n",
    "    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".KzDlHZ\"))  # Ensure the class name is correct\n",
    ")\n",
    "\n",
    "\n",
    "# i don't know why these variables have stopped working\n",
    "brand_names_list = []\n",
    "\n",
    "# when i had the data this was working well\n",
    "pattern = r'(\\w+) ([\\w\\s]+) \\(([\\w\\s]+), (\\d+ \\w+)\\)'\n",
    "\n",
    "# i have run out of time and don't know why the css selector has stopped working??\n",
    "brands = []\n",
    "models = []\n",
    "colours = []\n",
    "storages = []\n",
    "\n",
    "# i can't use the data loop as the variables don't contain any data\n",
    "\n",
    "#\n",
    "# for item in data:\n",
    "#    match = re.match(pattern, item)\n",
    "#    if match:\n",
    "#        brands.append(match.group(1))\n",
    "#        models.append(match.group(2))\n",
    "#        colours.append(match.group(3))\n",
    "#        storages.append(match.group(4))\n",
    "\n",
    "# Creating a DataFrame\n",
    "df1 = pd.DataFrame({\n",
    "    'Brand': brands,\n",
    "    'Model': models,\n",
    "    'Colour': colours,\n",
    "    'Storage': storages\n",
    "})\n",
    "\n",
    "# print(df1)\n",
    "# Loop through the elements and append their text to the list\n",
    "#for brand in brand_elements:\n",
    "#    brand_names_list.append(brand.text)\n",
    "    \n",
    "brand_tech = WebDriverWait(driver, 30).until(\n",
    "    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".G4BRas\"))\n",
    ")\n",
    "# Create a list to store product information\n",
    "product_data = []\n",
    "\n",
    "# Function to split the string into separate lines\n",
    "def split_features(data2):\n",
    "    return re.split(r'\\n', data2)\n",
    "\n",
    "# Extract text from each element and append to product_data\n",
    "for tech in brand_tech:\n",
    "    product_data.append(tech.text)\n",
    "\n",
    "# Split each string into separate lines and store in a list\n",
    "split_data = [split_features(data2) for data2 in product_data]\n",
    "\n",
    "# Create a DataFrame\n",
    "df2 = pd.DataFrame(split_data, columns=[0, 1, 2, 3, 4, 5])\n",
    "\n",
    "#Rename the columns in df2\n",
    "df2.columns = ['Memory', 'Display Size', 'Camera', 'Battery', 'Processor', 'Warranty']\n",
    "\n",
    "\n",
    "df_combined = pd.concat([df1, df2], axis=1)\n",
    "\n",
    "df_combined\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# as i mentioned the code stopped working as i had it completing the dataframe, with all the datas\n",
    "# i have run out of time to try and troubleshoot this. Thanks for reading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b182a957",
   "metadata": {},
   "source": [
    "# Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cdc7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup edge to load google maps\n",
    "driver = webdriver.Edge()\n",
    "driver.get('https://www.google.co.uk/maps/')\n",
    "\n",
    "# look for the accept all cookies button\n",
    "element = WebDriverWait(driver, 30).until(\n",
    "    EC.presence_of_element_located((By.CSS_SELECTOR, \"button[aria-label='Accept all']\"))\n",
    ")\n",
    "\n",
    "# Scroll to reveal the button\n",
    "driver.execute_script(\"arguments[0].scrollIntoView();\", element)\n",
    "# press the button\n",
    "element.submit()\n",
    "\n",
    "time.sleep(5)\n",
    "# locate the search bar field\n",
    "element1 = WebDriverWait(driver, 30).until(\n",
    "    EC.presence_of_element_located((By.CSS_SELECTOR, \".searchboxinput\"))\n",
    ")\n",
    "# Send the keys London\n",
    "element1.send_keys('London')\n",
    "# enter the letters\n",
    "element1.send_keys(Keys.RETURN)\n",
    "\n",
    "time.sleep(7)\n",
    "\n",
    "# the coordinates are found in the url\n",
    "current_url = driver.current_url\n",
    "print(\"Current URL:\", current_url)\n",
    "\n",
    "# use coordinates variable to regex the latitude and longitude\n",
    "# create the variables for latitude and longitude\n",
    "coordinates = current_url.split('@')[1].split(',')[:2]\n",
    "latitude = coordinates[0]\n",
    "longitude = coordinates[1].split(',')[0]\n",
    "\n",
    "print(f\"Latitude: {latitude}, Longitude: {longitude}\")\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# that was quite straightforward compared to the other questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e566bc",
   "metadata": {},
   "source": [
    "# Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c3e52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up edge to load digit\n",
    "driver = webdriver.Edge()\n",
    "driver.get('https://www.digit.in/')\n",
    "\n",
    "time.sleep(6)\n",
    "# load the cookie button\n",
    "element = WebDriverWait(driver, 30).until(\n",
    "    EC.presence_of_element_located((By.CSS_SELECTOR, \"button[aria-label='Consent']\"))\n",
    ")\n",
    "\n",
    "# Scroll the button\n",
    "driver.execute_script(\"arguments[0].scrollIntoView();\", element)\n",
    "# Click the button\n",
    "element.click()\n",
    "\n",
    "time.sleep(6)\n",
    "# load the fullscreen\n",
    "driver.fullscreen_window()\n",
    "# wait a few seconds\n",
    "time.sleep(6)\n",
    "\n",
    "# locate the search bar\n",
    "search_input = WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.ID, \"woocommerce-product-search-field-0\"))\n",
    ")   \n",
    "\n",
    "# input gaming laptops\n",
    "search_input.send_keys(\"Gaming laptops\")\n",
    "\n",
    "# press Enter key\n",
    "search_input.send_keys(Keys.RETURN)\n",
    "\n",
    "\n",
    "# i ran out of time on this question\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0486de",
   "metadata": {},
   "source": [
    "# 7. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: \n",
    "# “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd6bce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load edge to get forbes.com\n",
    "driver = webdriver.Edge()\n",
    "driver.get('https://www.forbes.com/')\n",
    "# time to load\n",
    "time.sleep(6)\n",
    "\n",
    "# locate the cookie button and click accept\n",
    "consent_button = WebDriverWait(driver, 30).until(\n",
    "    EC.presence_of_element_located((By.CSS_SELECTOR, \"button[aria-label='Accept All']\"))\n",
    ")\n",
    "driver.execute_script(\"arguments[0].scrollIntoView();\", consent_button)\n",
    "consent_button.click()\n",
    "\n",
    "# load time\n",
    "time.sleep(2)\n",
    "\n",
    "# full screen is easier to load search\n",
    "driver.fullscreen_window()\n",
    "time.sleep(6)\n",
    "\n",
    "try:\n",
    "    # locate billionaires article\n",
    "    billionaires_link = WebDriverWait(driver, 30).until(\n",
    "        EC.presence_of_element_located((By.LINK_TEXT, \"Billionaires\"))\n",
    "    )\n",
    "\n",
    "    # scroll loop is necessary to load billionaires\n",
    "    for _ in range(5):  \n",
    "        driver.execute_script(\"window.scrollBy(0, 800);\")  # by pixels to load more content\n",
    "        time.sleep(1)  # short delay\n",
    "    \n",
    "    # keep scrolling into view of billionaires link\n",
    "    try:\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", billionaires_link)\n",
    "        time.sleep(2)  # short break\n",
    "        driver.execute_script(\"arguments[0].click();\", billionaires_link)  # click the link\n",
    "    except Exception as click_error:\n",
    "        print(f\"Click error: {click_error}\")\n",
    "\n",
    "    time.sleep(7)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    \n",
    "    driver.fullscreen_window()\n",
    "\n",
    "# variable to load billionaires list  \n",
    "billion_button = WebDriverWait(driver, 60).until(\n",
    "    EC.presence_of_element_located((By.CSS_SELECTOR, \"a[aria-label=\\\"Forbes World's Billionaires List 2024\\\"]\"))\n",
    ")\n",
    "# Scroll page down until the end using a set pixel rate\n",
    "for _ in range(5):  \n",
    "        driver.execute_script(\"window.scrollBy(0, 800);\")  \n",
    "        time.sleep(1)  \n",
    "        \n",
    "# click the button        \n",
    "billion_button.click()  \n",
    "# fullscreen for more data\n",
    "driver.fullscreen_window()\n",
    "\n",
    "# short wait\n",
    "time.sleep(8)\n",
    "\n",
    "# create a variable to find data\n",
    "data_elements = driver.find_elements(By.CSS_SELECTOR, \"div[role='cell']\")\n",
    "\n",
    "# Extract text from each element and store in variable\n",
    "data_list = [element.text for element in data_elements]\n",
    "\n",
    "# create the data frame\n",
    "data_frame = pd.DataFrame(data_list)\n",
    "\n",
    "# create the columns for the dataframe\n",
    "data_frame = pd.DataFrame(columns=['Rank', 'Name', 'Networth', 'Age', 'Country', 'Source', 'Industry'])\n",
    "\n",
    "# Use the full screen to load all 7 elements using some regex loop automatically into columns\n",
    "for i in range(0, len(data_list), 7):\n",
    "    new_row = pd.Series(data_list[i:i+7], index=data_frame.columns)\n",
    "    data_frame = pd.concat([data_frame, new_row.to_frame().T], ignore_index=True)\n",
    "# Print the DataFrame\n",
    "\n",
    "print(data_frame)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9800ee27",
   "metadata": {},
   "source": [
    "# 8. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc10f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets go to youtube\n",
    "driver = webdriver.Edge()\n",
    "driver.get('https://www.youtube.com/')\n",
    "\n",
    "time.sleep(6)\n",
    "driver.fullscreen_window()\n",
    "\n",
    "\n",
    "# start to look for the cookie button\n",
    "accept_all_button = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.XPATH, \"//span[text()='Accept all']\"))\n",
    ")\n",
    "\n",
    "# Scroll for the button\n",
    "driver.execute_script(\"arguments[0].click();\", accept_all_button)\n",
    "# short rest\n",
    "time.sleep(4)\n",
    "\n",
    "# look for the search bar\n",
    "search_input_div = WebDriverWait(driver, 10).until(\n",
    "    EC.visibility_of_element_located((By.ID, \"search-input\"))\n",
    ")\n",
    "driver.execute_script(\"arguments[0].scrollIntoView(true);\", search_input_div)\n",
    "# click\n",
    "search_input_div.click()\n",
    "# Optional: Wait for a brief moment\n",
    "time.sleep(2)\n",
    "\n",
    "# Go fullscreen\n",
    "driver.fullscreen_window()\n",
    "time.sleep(6)\n",
    "# look for the search bar and click\n",
    "search_input = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.XPATH, \"//div[@id='search-input']//input\"))\n",
    ")\n",
    "search_input.click()\n",
    "\n",
    "# input video title\n",
    "search_input.send_keys(\"Billie Eilish - BIRDS OF A FEATHER (Official Lyric Video)\")\n",
    "\n",
    "# press the Enter key\n",
    "search_input.send_keys(Keys.RETURN)\n",
    "\n",
    "# create the variable and look for the video on the youtube page\n",
    "video_title_link = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.XPATH, \"//a[@title='Billie Eilish - BIRDS OF A FEATHER (Official Lyric Video)']\"))\n",
    ")\n",
    "# short rest\n",
    "time.sleep(15)\n",
    "# Scroll the link into view and click it\n",
    "driver.execute_script(\"window.scrollBy(0, -100);\")  # Adjust the scroll position if needed\n",
    "\n",
    "# click the video variable\n",
    "video_title_link.click()\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(10)\n",
    "\n",
    "# shrink page to load comments\n",
    "driver.set_window_size(800, 600)\n",
    "\n",
    "# method to scroll down the page\n",
    "def scroll_down(driver):\n",
    "    driver.execute_script(\"window.scrollBy(0, 5000);\")  # Increase scroll amount to 5000 pixels\n",
    "    time.sleep(2)\n",
    "\n",
    "time.sleep(10)\n",
    "# keep scrolling variable loop\n",
    "max_attempts = 60  # Set a maximum number of scroll attempts\n",
    "attempts = 0\n",
    "\n",
    "while attempts < max_attempts:\n",
    "    scroll_down(driver)\n",
    "    attempts += 1\n",
    "\n",
    "# look in the html for the comments\n",
    "page_source = driver.page_source\n",
    "soup8 = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "# create the variable save the span class data\n",
    "spans = soup8.find_all('span', class_='yt-core-attributed-string yt-core-attributed-string--white-space-pre-wrap')\n",
    "\n",
    "# Create the variable to extract the text from the data 24th to the 524th comment and save to a list\n",
    "comments = [span.get_text() for span in spans[23:524]]\n",
    "\n",
    "# create the dataframe variable with column comment\n",
    "df8 = pd.DataFrame(comments, columns=['Comment'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df8.to_csv('comments.csv', index=False)\n",
    "\n",
    "# finally print\n",
    "print(df8)\n",
    "    \n",
    "driver.quit()    \n",
    "\n",
    "# Thanks for reading, i havent webscraped for the upvote and date posted in this notebook file.\n",
    "# I did work out that data but ran out of time to combine the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151f042a",
   "metadata": {},
   "source": [
    "# 9. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74076da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load edge and go to hostelworld\n",
    "driver = webdriver.Edge()\n",
    "driver.get('https://www.hostelworld.com/')\n",
    "\n",
    "time.sleep(6)\n",
    "# wait for the accept cookie button to appear\n",
    "driver.fullscreen_window()\n",
    "accept_all_button = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.XPATH, \"//*[@id='truste-consent-button']\"))\n",
    ")\n",
    "# click the accept button\n",
    "driver.execute_script(\"arguments[0].click();\", accept_all_button)\n",
    "\n",
    "time.sleep(25)\n",
    "# create the variable to look for the London link\n",
    "element = driver.find_element(By.CSS_SELECTOR, 'a[href=\"https://www.hostelworld.com/st/hostels/europe/england/london/\"]')\n",
    "\n",
    "# Scroll the page until london variable loads\n",
    "driver.execute_script(\"arguments[0].scrollIntoView(true);\", element)\n",
    "\n",
    "# Click the london variable button\n",
    "element.click()\n",
    "\n",
    "#variables to save the data in the span classes\n",
    "hostel_location = driver.find_elements(By.CSS_SELECTOR, 'span[data-v-466b48c5]')\n",
    "\n",
    "# Extract and print the text from each span element\n",
    "for span in hostel_location:\n",
    "    print(span.text)\n",
    "\n",
    "hostel_reviews = driver.find_elements(By.CSS_SELECTOR, 'span[data-v-7b10f35f]')\n",
    "\n",
    "for span1 in hostel_reviews:\n",
    "    print(span1.text)\n",
    "    \n",
    "hostel_prices = driver.find_elements(By.CSS_SELECTOR, 'strong[data-v-466b48c5]')\n",
    "\n",
    "for span2 in hostel_prices:\n",
    "    print(span2.text)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# thanks for reading, i managed to webscrape the data but ran out of time to create a dataframe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
